{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "ADPET68xjlzr",
        "tags": []
      },
      "source": [
        "# 第10章: 事前学習済み言語モデル（GPT型）\n",
        "\n",
        "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "C1xKmMckti92",
        "tags": []
      },
      "source": [
        "## 90. 次単語予測\n",
        "\n",
        "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. トークナイザーとモデルのロード\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# 2. プロンプトをトークナイズ\n",
        "prompt = \"The movie was full of\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")  # shape: [1, seq_len]\n",
        "\n",
        "# トークンID列を確認\n",
        "print(\"Token IDs:\", input_ids[0].tolist())\n",
        "print(\"Tokens:\", [tokenizer.decode([tid]) for tid in input_ids[0]],\"\\n\")\n",
        "\n",
        "# 3. モデルに入力して出力（次のトークンのロジットを取得）\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
        "\n",
        "# 最後のトークンの次のトークンの確率分布\n",
        "next_token_logits = logits[0, -1, :]  # shape: [vocab_size]\n",
        "\n",
        "# 4. 確率化（ソフトマックス）\n",
        "probs = F.softmax(next_token_logits, dim=0)\n",
        "\n",
        "# 5. 上位10個トークンを取得\n",
        "topk = torch.topk(probs, k=10)\n",
        "topk_probs = topk.values.tolist()\n",
        "topk_indices = topk.indices.tolist()\n",
        "\n",
        "print(\"Top 10 next tokens and their probabilities:\")\n",
        "for idx, prob in zip(topk_indices, topk_probs):\n",
        "    print(f\"Token: '{tokenizer.decode([idx])}' (ID: {idx}), Probability: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lhD2l39SKob",
        "outputId": "e0d6dcf7-d6b2-4c93-8a87-990e145a7d8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [464, 3807, 373, 1336, 286]\n",
            "Tokens: ['The', ' movie', ' was', ' full', ' of'] \n",
            "\n",
            "Top 10 next tokens and their probabilities:\n",
            "Token: ' jokes' (ID: 14532), Probability: 0.0219\n",
            "Token: ' great' (ID: 1049), Probability: 0.0186\n",
            "Token: ' laughs' (ID: 22051), Probability: 0.0115\n",
            "Token: ' bad' (ID: 2089), Probability: 0.0109\n",
            "Token: ' surprises' (ID: 24072), Probability: 0.0107\n",
            "Token: ' references' (ID: 10288), Probability: 0.0105\n",
            "Token: ' fun' (ID: 1257), Probability: 0.0100\n",
            "Token: ' humor' (ID: 14733), Probability: 0.0074\n",
            "Token: ' \"' (ID: 366), Probability: 0.0074\n",
            "Token: ' the' (ID: 262), Probability: 0.0067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "s1RhOldA0meh",
        "tags": []
      },
      "source": [
        "## 91. 続きのテキストの予測\n",
        "\n",
        "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# GPT-2にはpad_token_idがない → eos_token_id (50256) を代用する\n",
        "pad_token_id = tokenizer.eos_token_id\n",
        "attention_mask = input_ids.ne(pad_token_id).long()  # すべて1（パディングしていない）になるがOK\n",
        "\n",
        "def generate_text(method=\"greedy\", temperature=1.0, top_k=50, top_p=0.9, num_beams=3, max_length=50):\n",
        "    generation_args = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"pad_token_id\": pad_token_id,\n",
        "        \"max_length\": max_length\n",
        "    }\n",
        "\n",
        "    if method == \"greedy\":\n",
        "        generation_args[\"do_sample\"] = False\n",
        "    elif method == \"beam\":\n",
        "        generation_args.update({\"do_sample\": False, \"num_beams\": num_beams, \"early_stopping\": True})\n",
        "    elif method == \"top-k\":\n",
        "        generation_args.update({\"do_sample\": True, \"top_k\": top_k, \"temperature\": temperature})\n",
        "    elif method == \"top-p\":\n",
        "        generation_args.update({\"do_sample\": True, \"top_p\": top_p, \"temperature\": temperature})\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported decoding method\")\n",
        "\n",
        "    outputs = model.generate(**generation_args)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 動作確認\n",
        "print(\"=== Top-p Sampling (p=0.9, temp=1.0) ===\")   #常に一番確率の高いトークンを選択。決定的な結果で多様性がない。\n",
        "print(generate_text(method=\"top-p\", top_p=0.9, temperature=1.0))\n",
        "\n",
        "print(\"\\n=== Greedy Decoding ===\")    #複数候補（ビーム幅）を保持しつつ、全体の尤度が高いものを選ぶ。Greedyよりは多様性が出るが、似た表現に偏ることが多い\n",
        "print(generate_text(method=\"greedy\"))\n",
        "\n",
        "print(\"\\n=== Beam Search (num_beams=5) ===\")    #次トークンの上位k個からランダムに選ぶ。多様性が出やすい\n",
        "print(generate_text(method=\"beam\", num_beams=5))\n",
        "\n",
        "print(\"\\n=== Top-k Sampling (k=50, temp=1.0) ===\")    #確率の累積がpになるまでのトークンからランダムに選ぶ。動的に候補数が変わり、多様性と品質のバランスが良い\n",
        "print(generate_text(method=\"top-k\", top_k=50, temperature=1.0))\n",
        "\n",
        "print(\"\\n=== Top-p Sampling (p=0.9, temp=0.5) ===\")   #分布を鋭くして「確率の高いトークンをより選びやすく」\n",
        "print(generate_text(method=\"top-p\", top_p=0.9, temperature=0.5))\n",
        "\n",
        "print(\"\\n=== Top-p Sampling (p=0.9, temp=1.5) ===\")   #分布を平坦にして「多様性が増す」\n",
        "print(generate_text(method=\"top-p\", top_p=0.9, temperature=1.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5r9SEYMS7Km",
        "outputId": "58f0f0f3-528f-4596-dd81-a2e8fcea51ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top-p Sampling (p=0.9, temp=1.0) ===\n",
            "The movie was full of good laughs. Some of it was funny because of the characters, but all the time I think we're very proud of this team that we have. We know the way they play, we know how they play, and we\n",
            "\n",
            "=== Greedy Decoding ===\n",
            "The movie was full of jokes and jokes about how the movie was a joke. It was a joke about how the movie was a joke. It was a joke about how the movie was a joke. It was a joke about how the movie was a\n",
            "\n",
            "=== Beam Search (num_beams=5) ===\n",
            "The movie was full of jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes\n",
            "\n",
            "=== Top-k Sampling (k=50, temp=1.0) ===\n",
            "The movie was full of spoilers, and a lot of people thought that this all fit the movie. Well, in the end, I found out what happened, and they did a great job on the ending, and if I did it again this year\n",
            "\n",
            "=== Top-p Sampling (p=0.9, temp=0.5) ===\n",
            "The movie was full of the usual suspects: a young man, a young woman, a young man, a young man, a young man.\n",
            "\n",
            "The movie was full of the usual suspects: a young man, a young woman, a young\n",
            "\n",
            "=== Top-p Sampling (p=0.9, temp=1.5) ===\n",
            "The movie was full of great drama on the ground-play part. (One of their main strengths is that they had to build such believable characters, not show a certain type of character.) And in an earlier part of my career when I wrote such\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "7ZFadg6B8VdA",
        "tags": []
      },
      "source": [
        "## 92. 予測されたテキストの確率を計算\n",
        "\n",
        "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# モデルとトークナイザーの準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# 初期プロンプト\n",
        "prompt = \"The movie was full of\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "generated_ids = input_ids.clone()\n",
        "\n",
        "# 最大生成トークン数\n",
        "max_new_tokens = 10\n",
        "\n",
        "# 尤度を記録\n",
        "log_probs = []\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    # モデル出力\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=generated_ids)\n",
        "        logits = outputs.logits  # (1, seq_len, vocab_size)\n",
        "\n",
        "    # 最新トークンに対する確率分布\n",
        "    next_token_logits = logits[0, -1, :]  # (vocab_size,)\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Greedyで次トークンを選ぶ\n",
        "    next_token_id = torch.argmax(probs).unsqueeze(0)\n",
        "    log_prob_value = log_prob[next_token_id].item()\n",
        "    log_probs.append(log_prob_value)\n",
        "\n",
        "    # 生成文を更新\n",
        "    generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=1)\n",
        "\n",
        "# 結果表示\n",
        "generated_tokens = generated_ids[0]\n",
        "decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "new_tokens = generated_tokens[len(input_ids[0]):]\n",
        "\n",
        "print(\"=== 生成結果 ===\")\n",
        "print(decoded)\n",
        "\n",
        "print(\"\\n=== 各トークンと尤度 ===\")\n",
        "for token_id, lp in zip(new_tokens, log_probs):\n",
        "    token_str = tokenizer.decode(token_id)\n",
        "    print(f\"{token_str!r:>15} : log_prob = {lp:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYu-EbiNVqSr",
        "outputId": "6a91b076-b683-490d-89b7-38c85f14b04e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 生成結果 ===\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n",
            "\n",
            "=== 各トークンと尤度 ===\n",
            "       ' jokes' : log_prob = -3.8216\n",
            "         ' and' : log_prob = -1.2405\n",
            "       ' jokes' : log_prob = -2.3177\n",
            "       ' about' : log_prob = -1.5820\n",
            "         ' how' : log_prob = -2.3054\n",
            "         ' the' : log_prob = -2.4694\n",
            "       ' movie' : log_prob = -3.3129\n",
            "         ' was' : log_prob = -1.2162\n",
            "           ' a' : log_prob = -2.6930\n",
            "        ' joke' : log_prob = -1.7515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "FvNCTMj6OegF",
        "tags": []
      },
      "source": [
        "## 93. パープレキシティ\n",
        "\n",
        "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
        "\n",
        "+ The movie was full of surprises\n",
        "+ The movies were full of surprises\n",
        "+ The movie were full of surprises\n",
        "+ The movies was full of surprises\n",
        "\n",
        "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# モデル・トークナイザーの準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "def calculate_perplexity(sentence):\n",
        "    encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    input_ids = encodings.input_ids\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "# テスト文（文法正誤入り）\n",
        "sentences = [\n",
        "    \"The movie was full of surprises\",     # 正しい\n",
        "    \"The movies were full of surprises\",   # 正しい\n",
        "    \"The movie were full of surprises\",    # 主語-動詞の一致誤り\n",
        "    \"The movies was full of surprises\"     # 主語-動詞の一致誤り\n",
        "]\n",
        "\n",
        "# 各文に対するパープレキシティを表示\n",
        "print(\"=== パープレキシティ測定結果 ===\")   #パープレキシティ：ある言語モデルにとって、ある文がどれだけ「予測しやすいか」の尺度\n",
        "for sentence in sentences:\n",
        "    ppl = calculate_perplexity(sentence)\n",
        "    print(f\"{sentence:<40} : PPL = {ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZYNhiwIWEBf",
        "outputId": "bc1706a6-ee12-4ec4-db8d-632ecec405bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== パープレキシティ測定結果 ===\n",
            "The movie was full of surprises          : PPL = 99.35\n",
            "The movies were full of surprises        : PPL = 126.48\n",
            "The movie were full of surprises         : PPL = 278.88\n",
            "The movies was full of surprises         : PPL = 274.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-7fB-n9suYg"
      },
      "source": [
        "## 94. チャットテンプレート\n",
        "\n",
        "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# モデルとトークナイザーの準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# チャットテンプレートに基づくプロンプト作成\n",
        "prompt = (\n",
        "    \"<|system|>\\nYou are a helpful assistant.\\n\"\n",
        "    \"<|user|>\\nWhat do you call a sweet eaten after dinner?\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "\n",
        "# トークン化\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 応答生成（Top-pサンプリング）\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=input_ids.shape[1] + 30,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.8,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# 応答部分を抽出\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "response = generated_text.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "# 出力\n",
        "print(\"=== 応答 ===\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "dak88vQqW2JM",
        "outputId": "62312452-7284-4be9-917e-a673b6a37781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 応答 ===\n",
            "What do you call a sweet eaten after dinner?\n",
            "\n",
            "<|your|>\n",
            "What do you call a sweet eaten after dinner?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "PT-bk0XWIZ2E",
        "tags": []
      },
      "source": [
        "## 95. マルチターンのチャット\n",
        "\n",
        "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# モデルとトークナイザーの読み込み\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# チャット風プロンプトの構築\n",
        "prompt = (\n",
        "    \"<|system|>\\nYou are a helpful assistant.\\n\"\n",
        "    \"<|user|>\\nWhat do you call a sweet eaten after dinner?\\n\"\n",
        "    \"<|assistant|>\\nWhat do you call a sweet eaten after dinner?\\n\"\n",
        "    \"<|user|>\\nPlease give me the plural form of the word with its spelling in reverse order.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "\n",
        "# トークナイズ\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 応答生成\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=input_ids.shape[1] + 40,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.8,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# 応答部分の抽出\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "response = generated_text.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "# 表示\n",
        "print(\"=== 与えたプロンプト ===\")\n",
        "print(prompt)\n",
        "print(\"\\n=== モデルの応答 ===\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "v8yW3e_OXgUh",
        "outputId": "44c7c109-c8f9-4e81-d2b5-1e81c282aa17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 与えたプロンプト ===\n",
            "<|system|>\n",
            "You are a helpful assistant.\n",
            "<|user|>\n",
            "What do you call a sweet eaten after dinner?\n",
            "<|assistant|>\n",
            "What do you call a sweet eaten after dinner?\n",
            "<|user|>\n",
            "Please give me the plural form of the word with its spelling in reverse order.\n",
            "<|assistant|>\n",
            "\n",
            "\n",
            "=== モデルの応答 ===\n",
            "What do you call a sweet eaten after dinner?\n",
            "<|user|>\n",
            "Please give me the plural form of the word with its spelling in reverse order.\n",
            "<|assistant|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "qH0YortL0afd",
        "tags": []
      },
      "source": [
        "## 96. プロンプトによる感情分析\n",
        "\n",
        "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "giA6FivrKaSf",
        "tags": []
      },
      "source": [
        "## 97. 埋め込みに基づく感情分析\n",
        "\n",
        "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "UnREZD3nTWUr",
        "tags": []
      },
      "source": [
        "## 98. ファインチューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "4f0St5Ce0l34",
        "tags": []
      },
      "source": [
        "## 99. 選好チューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}