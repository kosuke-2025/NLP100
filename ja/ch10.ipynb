{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "ADPET68xjlzr",
        "tags": []
      },
      "source": [
        "# 第10章: 事前学習済み言語モデル（GPT型）\n",
        "\n",
        "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "C1xKmMckti92",
        "tags": []
      },
      "source": [
        "## 90. 次単語予測\n",
        "\n",
        "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. トークナイザーとモデルのロード\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# 2. プロンプトをトークナイズ\n",
        "prompt = \"The movie was full of\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")  # shape: [1, seq_len]\n",
        "\n",
        "# トークンID列を確認\n",
        "print(\"Token IDs:\", input_ids[0].tolist())\n",
        "print(\"Tokens:\", [tokenizer.decode([tid]) for tid in input_ids[0]],\"\\n\")\n",
        "\n",
        "# 3. モデルに入力して出力（次のトークンのロジットを取得）\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
        "\n",
        "# 最後のトークンの次のトークンの確率分布\n",
        "next_token_logits = logits[0, -1, :]  # shape: [vocab_size]\n",
        "\n",
        "# 4. 確率化（ソフトマックス）\n",
        "probs = F.softmax(next_token_logits, dim=0)\n",
        "\n",
        "# 5. 上位10個トークンを取得\n",
        "topk = torch.topk(probs, k=10)\n",
        "topk_probs = topk.values.tolist()\n",
        "topk_indices = topk.indices.tolist()\n",
        "\n",
        "print(\"Top 10 next tokens and their probabilities:\")\n",
        "for idx, prob in zip(topk_indices, topk_probs):\n",
        "    print(f\"Token: '{tokenizer.decode([idx])}' (ID: {idx}), Probability: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lhD2l39SKob",
        "outputId": "e0d6dcf7-d6b2-4c93-8a87-990e145a7d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [464, 3807, 373, 1336, 286]\n",
            "Tokens: ['The', ' movie', ' was', ' full', ' of'] \n",
            "\n",
            "Top 10 next tokens and their probabilities:\n",
            "Token: ' jokes' (ID: 14532), Probability: 0.0219\n",
            "Token: ' great' (ID: 1049), Probability: 0.0186\n",
            "Token: ' laughs' (ID: 22051), Probability: 0.0115\n",
            "Token: ' bad' (ID: 2089), Probability: 0.0109\n",
            "Token: ' surprises' (ID: 24072), Probability: 0.0107\n",
            "Token: ' references' (ID: 10288), Probability: 0.0105\n",
            "Token: ' fun' (ID: 1257), Probability: 0.0100\n",
            "Token: ' humor' (ID: 14733), Probability: 0.0074\n",
            "Token: ' \"' (ID: 366), Probability: 0.0074\n",
            "Token: ' the' (ID: 262), Probability: 0.0067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "s1RhOldA0meh",
        "tags": []
      },
      "source": [
        "## 91. 続きのテキストの予測\n",
        "\n",
        "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# GPT-2にはpad_token_idがない → eos_token_id (50256) を代用する\n",
        "pad_token_id = tokenizer.eos_token_id\n",
        "attention_mask = input_ids.ne(pad_token_id).long()  # すべて1（パディングしていない）になるがOK\n",
        "\n",
        "def generate_text(method=\"greedy\", temperature=1.0, top_k=50, top_p=0.9, num_beams=3, max_length=50):\n",
        "    generation_args = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"pad_token_id\": pad_token_id,\n",
        "        \"max_length\": max_length\n",
        "    }\n",
        "\n",
        "    if method == \"greedy\":\n",
        "        generation_args[\"do_sample\"] = False\n",
        "    elif method == \"beam\":\n",
        "        generation_args.update({\"do_sample\": False, \"num_beams\": num_beams, \"early_stopping\": True})\n",
        "    elif method == \"top-k\":\n",
        "        generation_args.update({\"do_sample\": True, \"top_k\": top_k, \"temperature\": temperature})\n",
        "    elif method == \"top-p\":\n",
        "        generation_args.update({\"do_sample\": True, \"top_p\": top_p, \"temperature\": temperature})\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported decoding method\")\n",
        "\n",
        "    outputs = model.generate(**generation_args)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 動作確認\n",
        "print(\"=== Top-p Sampling (p=0.9, temp=1.0) ===\")   #常に一番確率の高いトークンを選択。決定的な結果で多様性がない。\n",
        "print(generate_text(method=\"top-p\", top_p=0.9, temperature=1.0))\n",
        "\n",
        "print(\"\\n=== Greedy Decoding ===\")    #複数候補（ビーム幅）を保持しつつ、全体の尤度が高いものを選ぶ。Greedyよりは多様性が出るが、似た表現に偏ることが多い\n",
        "print(generate_text(method=\"greedy\"))\n",
        "\n",
        "print(\"\\n=== Beam Search (num_beams=5) ===\")    #次トークンの上位k個からランダムに選ぶ。多様性が出やすい\n",
        "print(generate_text(method=\"beam\", num_beams=5))\n",
        "\n",
        "print(\"\\n=== Top-k Sampling (k=50, temp=1.0) ===\")    #確率の累積がpになるまでのトークンからランダムに選ぶ。動的に候補数が変わり、多様性と品質のバランスが良い\n",
        "print(generate_text(method=\"top-k\", top_k=50, temperature=1.0))\n",
        "\n",
        "print(\"\\n=== Top-p Sampling (p=0.9, temp=0.5) ===\")   #分布を鋭くして「確率の高いトークンをより選びやすく」\n",
        "print(generate_text(method=\"top-p\", top_p=0.9, temperature=0.5))\n",
        "\n",
        "print(\"\\n=== Top-p Sampling (p=0.9, temp=1.5) ===\")   #分布を平坦にして「多様性が増す」\n",
        "print(generate_text(method=\"top-p\", top_p=0.9, temperature=1.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5r9SEYMS7Km",
        "outputId": "58f0f0f3-528f-4596-dd81-a2e8fcea51ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top-p Sampling (p=0.9, temp=1.0) ===\n",
            "The movie was full of good laughs. Some of it was funny because of the characters, but all the time I think we're very proud of this team that we have. We know the way they play, we know how they play, and we\n",
            "\n",
            "=== Greedy Decoding ===\n",
            "The movie was full of jokes and jokes about how the movie was a joke. It was a joke about how the movie was a joke. It was a joke about how the movie was a joke. It was a joke about how the movie was a\n",
            "\n",
            "=== Beam Search (num_beams=5) ===\n",
            "The movie was full of jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes\n",
            "\n",
            "=== Top-k Sampling (k=50, temp=1.0) ===\n",
            "The movie was full of spoilers, and a lot of people thought that this all fit the movie. Well, in the end, I found out what happened, and they did a great job on the ending, and if I did it again this year\n",
            "\n",
            "=== Top-p Sampling (p=0.9, temp=0.5) ===\n",
            "The movie was full of the usual suspects: a young man, a young woman, a young man, a young man, a young man.\n",
            "\n",
            "The movie was full of the usual suspects: a young man, a young woman, a young\n",
            "\n",
            "=== Top-p Sampling (p=0.9, temp=1.5) ===\n",
            "The movie was full of great drama on the ground-play part. (One of their main strengths is that they had to build such believable characters, not show a certain type of character.) And in an earlier part of my career when I wrote such\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "7ZFadg6B8VdA",
        "tags": []
      },
      "source": [
        "## 92. 予測されたテキストの確率を計算\n",
        "\n",
        "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# モデルとトークナイザーの準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# 初期プロンプト\n",
        "prompt = \"The movie was full of\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "generated_ids = input_ids.clone()\n",
        "\n",
        "# 最大生成トークン数\n",
        "max_new_tokens = 10\n",
        "\n",
        "# 尤度を記録\n",
        "log_probs = []\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    # モデル出力\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=generated_ids)\n",
        "        logits = outputs.logits  # (1, seq_len, vocab_size)\n",
        "\n",
        "    # 最新トークンに対する確率分布\n",
        "    next_token_logits = logits[0, -1, :]  # (vocab_size,)\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Greedyで次トークンを選ぶ\n",
        "    next_token_id = torch.argmax(probs).unsqueeze(0)\n",
        "    log_prob_value = log_prob[next_token_id].item()\n",
        "    log_probs.append(log_prob_value)\n",
        "\n",
        "    # 生成文を更新\n",
        "    generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=1)\n",
        "\n",
        "# 結果表示\n",
        "generated_tokens = generated_ids[0]\n",
        "decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "new_tokens = generated_tokens[len(input_ids[0]):]\n",
        "\n",
        "print(\"=== 生成結果 ===\")\n",
        "print(decoded)\n",
        "\n",
        "print(\"\\n=== 各トークンと尤度 ===\")\n",
        "for token_id, lp in zip(new_tokens, log_probs):\n",
        "    token_str = tokenizer.decode(token_id)\n",
        "    print(f\"{token_str!r:>15} : log_prob = {lp:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYu-EbiNVqSr",
        "outputId": "6a91b076-b683-490d-89b7-38c85f14b04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 生成結果 ===\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n",
            "\n",
            "=== 各トークンと尤度 ===\n",
            "       ' jokes' : log_prob = -3.8216\n",
            "         ' and' : log_prob = -1.2405\n",
            "       ' jokes' : log_prob = -2.3177\n",
            "       ' about' : log_prob = -1.5820\n",
            "         ' how' : log_prob = -2.3054\n",
            "         ' the' : log_prob = -2.4694\n",
            "       ' movie' : log_prob = -3.3129\n",
            "         ' was' : log_prob = -1.2162\n",
            "           ' a' : log_prob = -2.6930\n",
            "        ' joke' : log_prob = -1.7515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "FvNCTMj6OegF",
        "tags": []
      },
      "source": [
        "## 93. パープレキシティ\n",
        "\n",
        "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
        "\n",
        "+ The movie was full of surprises\n",
        "+ The movies were full of surprises\n",
        "+ The movie were full of surprises\n",
        "+ The movies was full of surprises\n",
        "\n",
        "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# モデル・トークナイザーの準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "def calculate_perplexity(sentence):\n",
        "    encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    input_ids = encodings.input_ids\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "# テスト文（文法正誤入り）\n",
        "sentences = [\n",
        "    \"The movie was full of surprises\",     # 正しい\n",
        "    \"The movies were full of surprises\",   # 正しい\n",
        "    \"The movie were full of surprises\",    # 主語-動詞の一致誤り\n",
        "    \"The movies was full of surprises\"     # 主語-動詞の一致誤り\n",
        "]\n",
        "\n",
        "# 各文に対するパープレキシティを表示\n",
        "print(\"=== パープレキシティ測定結果 ===\")   #パープレキシティ：ある言語モデルにとって、ある文がどれだけ「予測しやすいか」の尺度\n",
        "for sentence in sentences:\n",
        "    ppl = calculate_perplexity(sentence)\n",
        "    print(f\"{sentence:<40} : PPL = {ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZYNhiwIWEBf",
        "outputId": "bc1706a6-ee12-4ec4-db8d-632ecec405bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== パープレキシティ測定結果 ===\n",
            "The movie was full of surprises          : PPL = 99.35\n",
            "The movies were full of surprises        : PPL = 126.48\n",
            "The movie were full of surprises         : PPL = 278.88\n",
            "The movies was full of surprises         : PPL = 274.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-7fB-n9suYg"
      },
      "source": [
        "## 94. チャットテンプレート\n",
        "\n",
        "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# モデルとトークナイザーの準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# チャットテンプレートに基づくプロンプト作成\n",
        "prompt = (\n",
        "    \"<|system|>\\nYou are a helpful assistant.\\n\"\n",
        "    \"<|user|>\\nWhat do you call a sweet eaten after dinner?\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "\n",
        "# トークン化\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 応答生成（Top-pサンプリング）\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=input_ids.shape[1] + 30,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.8,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# 応答部分を抽出\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "response = generated_text.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "# 出力\n",
        "print(\"=== 応答 ===\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "dak88vQqW2JM",
        "outputId": "62312452-7284-4be9-917e-a673b6a37781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 応答 ===\n",
            "What do you call a sweet eaten after dinner?\n",
            "\n",
            "<|your|>\n",
            "What do you call a sweet eaten after dinner?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "PT-bk0XWIZ2E",
        "tags": []
      },
      "source": [
        "## 95. マルチターンのチャット\n",
        "\n",
        "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# モデルとトークナイザーの読み込み\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# チャット風プロンプトの構築\n",
        "prompt = (\n",
        "    \"<|system|>\\nYou are a helpful assistant.\\n\"\n",
        "    \"<|user|>\\nWhat do you call a sweet eaten after dinner?\\n\"\n",
        "    \"<|assistant|>\\nWhat do you call a sweet eaten after dinner?\\n\"\n",
        "    \"<|user|>\\nPlease give me the plural form of the word with its spelling in reverse order.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "\n",
        "# トークナイズ\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 応答生成\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=input_ids.shape[1] + 40,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.8,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# 応答部分の抽出\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "response = generated_text.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "# 表示\n",
        "print(\"=== 与えたプロンプト ===\")\n",
        "print(prompt)\n",
        "print(\"\\n=== モデルの応答 ===\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "v8yW3e_OXgUh",
        "outputId": "44c7c109-c8f9-4e81-d2b5-1e81c282aa17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 与えたプロンプト ===\n",
            "<|system|>\n",
            "You are a helpful assistant.\n",
            "<|user|>\n",
            "What do you call a sweet eaten after dinner?\n",
            "<|assistant|>\n",
            "What do you call a sweet eaten after dinner?\n",
            "<|user|>\n",
            "Please give me the plural form of the word with its spelling in reverse order.\n",
            "<|assistant|>\n",
            "\n",
            "\n",
            "=== モデルの応答 ===\n",
            "What do you call a sweet eaten after dinner?\n",
            "<|user|>\n",
            "Please give me the plural form of the word with its spelling in reverse order.\n",
            "<|assistant|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "qH0YortL0afd",
        "tags": []
      },
      "source": [
        "## 96. プロンプトによる感情分析\n",
        "\n",
        "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "metadata": {
        "id": "VCkKqYUDrQLp",
        "outputId": "6201ffed-59f9-497e-8318-d8244f608c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-26 03:52:55--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.14, 3.163.189.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7439277 (7.1M) [application/zip]\n",
            "Saving to: ‘SST-2.zip’\n",
            "\n",
            "SST-2.zip           100%[===================>]   7.09M  39.8MB/s    in 0.2s    \n",
            "\n",
            "2025-05-26 03:52:56 (39.8 MB/s) - ‘SST-2.zip’ saved [7439277/7439277]\n",
            "\n",
            "Archive:  SST-2.zip\n",
            "   creating: SST-2/\n",
            "  inflating: SST-2/dev.tsv           \n",
            "   creating: SST-2/original/\n",
            "  inflating: SST-2/original/README.txt  \n",
            "  inflating: SST-2/original/SOStr.txt  \n",
            "  inflating: SST-2/original/STree.txt  \n",
            "  inflating: SST-2/original/datasetSentences.txt  \n",
            "  inflating: SST-2/original/datasetSplit.txt  \n",
            "  inflating: SST-2/original/dictionary.txt  \n",
            "  inflating: SST-2/original/original_rt_snippets.txt  \n",
            "  inflating: SST-2/original/sentiment_labels.txt  \n",
            "  inflating: SST-2/test.tsv          \n",
            "  inflating: SST-2/train.tsv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# モデルとトークナイザーの準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# GPUが使えるなら使う\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# ゼロショットで感情を分類する関数\n",
        "def classify_sentiment(text):\n",
        "    prompt = f\"Review: {text}\\nSentiment:\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 選択肢トークン列\n",
        "    pos_ids = tokenizer.encode(\" Positive\", return_tensors=\"pt\")[0][1:].to(device)\n",
        "    neg_ids = tokenizer.encode(\" Negative\", return_tensors=\"pt\")[0][1:].to(device)\n",
        "\n",
        "    # Positive の尤度\n",
        "    pos_logprob = 0.0\n",
        "    current_input = input_ids.clone()\n",
        "    for token_id in pos_ids:\n",
        "        with torch.no_grad():\n",
        "            output = model(current_input)\n",
        "        log_probs = torch.nn.functional.log_softmax(output.logits[:, -1, :], dim=-1)\n",
        "        pos_logprob += log_probs[0, token_id].item()\n",
        "        current_input = torch.cat([current_input, token_id.view(1, 1)], dim=1)\n",
        "\n",
        "    # Negative の尤度\n",
        "    neg_logprob = 0.0\n",
        "    current_input = input_ids.clone()\n",
        "    for token_id in neg_ids:\n",
        "        with torch.no_grad():\n",
        "            output = model(current_input)\n",
        "        log_probs = torch.nn.functional.log_softmax(output.logits[:, -1, :], dim=-1)\n",
        "        neg_logprob += log_probs[0, token_id].item()\n",
        "        current_input = torch.cat([current_input, token_id.view(1, 1)], dim=1)\n",
        "\n",
        "    return 1 if pos_logprob > neg_logprob else 0\n",
        "\n",
        "# devデータで精度を評価\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for i, row in tqdm(dev_df.iterrows(), total=len(dev_df)):\n",
        "    sentence = row[\"sentence\"]\n",
        "    label = row[\"label\"]\n",
        "    pred = classify_sentiment(sentence)\n",
        "    if pred == label:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\nZero-shot accuracy on SST-2 dev set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "2cm0HVt7rVPS",
        "outputId": "ae4950f6-f9d6-448c-f11e-a24e19fc226d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 872/872 [00:00<00:00, 954.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zero-shot accuracy on SST-2 dev set: 0.4908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "giA6FivrKaSf",
        "tags": []
      },
      "source": [
        "## 97. 埋め込みに基づく感情分析\n",
        "\n",
        "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. データ読み込み\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
        "\n",
        "# 2. トークナイザー準備（パディングトークンを追加）\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2にはpad_tokenがないため代用\n",
        "\n",
        "# 3. Dataset定義\n",
        "class SST2Dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len=64):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.data.iloc[idx]['sentence']\n",
        "        label = self.data.iloc[idx]['label']\n",
        "        encoded = self.tokenizer(\n",
        "            sentence,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label)\n",
        "        }\n",
        "\n",
        "train_dataset = SST2Dataset(train_df, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 4. GPT2 + 分類器\n",
        "class GPT2ForClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.gpt2.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # 最後のトークンの出力（GPTは右から左への予測モデルなのでこれで良い）\n",
        "        last_token_output = outputs.last_hidden_state[:, -1, :]\n",
        "        logits = self.classifier(self.dropout(last_token_output))\n",
        "        return logits\n",
        "\n",
        "# 5. 学習設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2ForClassification().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 6. 学習ループ（1エポック）\n",
        "model.train()\n",
        "for batch in tqdm(train_loader):\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"label\"].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"✅ GPTベース分類モデルの学習が完了しました。\")\n"
      ],
      "metadata": {
        "id": "caxeXve0tC8h",
        "outputId": "f17eb010-8ce0-4ff2-fa1e-f1201e6ad5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4210/4210 [14:51<00:00,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPTベース分類モデルの学習が完了しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "UnREZD3nTWUr",
        "tags": []
      },
      "source": [
        "## 98. ファインチューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# データ読み込み\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
        "\n",
        "# ラベルマッピング\n",
        "label_map = {0: \"negative\", 1: \"positive\"}\n",
        "train_df[\"label_text\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "# トークナイザー・モデル準備\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad_token未定義のため設定\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "# データセット定義\n",
        "class SST2Dataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=64):\n",
        "        self.texts = [\n",
        "            f\"User: {row['sentence']}\\nAssistant: {row['label_text']}\"\n",
        "            for _, row in df.iterrows()\n",
        "        ]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        # GPT2の言語モデルトレーニングでは、labelsはinput_idsそのまま\n",
        "        labels = input_ids.clone()\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "# データローダー\n",
        "train_dataset = SST2Dataset(train_df, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# 最適化手法\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# 学習ループ\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "_SuZiprRy2D2",
        "outputId": "164339b5-6853-498c-f545-1762b34cbbf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 8419/8419 [23:52<00:00,  5.88it/s, loss=0.441]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Loss: 0.7031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 8419/8419 [23:52<00:00,  5.88it/s, loss=0.39]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Average Loss: 0.4673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "4f0St5Ce0l34",
        "tags": []
      },
      "source": [
        "## 99. 選好チューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. データ準備（SST-2）\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
        "label_map = {1: \"positive\", 0: \"negative\"}\n",
        "\n",
        "# 2. 選好データを作成\n",
        "prompts, chosen, rejected = [], [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    sentence = row[\"sentence\"]\n",
        "    true_label = label_map[row[\"label\"]]\n",
        "    wrong_label = \"negative\" if true_label == \"positive\" else \"positive\"\n",
        "    prompt = f\"User: {sentence}\\nAssistant:\"\n",
        "    prompts.append(prompt)\n",
        "    chosen.append(f\"{prompt} {true_label}\")\n",
        "    rejected.append(f\"{prompt} {wrong_label}\")\n",
        "\n",
        "# 3. トークナイザーとモデル\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = model.device\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "# 4. DPOっぽい学習\n",
        "batch_size = 8\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
        "        batch_chosen = chosen[i:i+batch_size]\n",
        "        batch_rejected = rejected[i:i+batch_size]\n",
        "\n",
        "        loss = 0\n",
        "        for ch, rej in zip(batch_chosen, batch_rejected):\n",
        "            # Encode\n",
        "            ch_ids = tokenizer(ch, return_tensors=\"pt\", padding=True).to(device)\n",
        "            rej_ids = tokenizer(rej, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "            # Get log probabilities\n",
        "            with torch.no_grad():\n",
        "                ch_outputs = model(**ch_ids)\n",
        "                rej_outputs = model(**rej_ids)\n",
        "\n",
        "            ch_logits = ch_outputs.logits[:, :-1, :]\n",
        "            rej_logits = rej_outputs.logits[:, :-1, :]\n",
        "\n",
        "            ch_labels = ch_ids.input_ids[:, 1:]\n",
        "            rej_labels = rej_ids.input_ids[:, 1:]\n",
        "\n",
        "            ch_loss = loss_fn(ch_logits.reshape(-1, ch_logits.size(-1)), ch_labels.reshape(-1)).mean()\n",
        "            rej_loss = loss_fn(rej_logits.reshape(-1, rej_logits.size(-1)), rej_labels.reshape(-1)).mean()\n",
        "\n",
        "            # DPO-style contrastive loss: minimize chosen loss, maximize rejected loss\n",
        "            pair_loss = ch_loss - rej_loss\n",
        "            loss += pair_loss\n",
        "\n",
        "        loss = loss / batch_size\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Avg DPO Loss: {total_loss / (len(prompts) // batch_size):.4f}\")\n"
      ],
      "metadata": {
        "id": "HlMTMxksuBN_",
        "outputId": "d6b4e428-a2db-4072-98da-bc119ecf38dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8419 [00:08<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1d1a37aeb13c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}