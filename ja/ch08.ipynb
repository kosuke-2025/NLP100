{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e1YwuFtZd1t",
      "metadata": {
        "editable": true,
        "id": "1e1YwuFtZd1t",
        "tags": []
      },
      "source": [
        "# 第8章: ニューラルネット\n",
        "\n",
        "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "import numpy\n",
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "snDcKzyym8CD",
        "outputId": "a456218d-bf4c-4f31-b4ad-631000a81250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "snDcKzyym8CD",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
      "metadata": {
        "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
      },
      "source": [
        "## 70. 単語埋め込みの読み込み\n",
        "\n",
        "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
        "\n",
        "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
        "\n",
        "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "v_size = len(model.key_to_index)  #model.key_to_index : 単語を数字に変換するための辞書   v_size : 300万単語\n",
        "d_emb = model.vector_size #d_emb : 300次元 [1,2,3,...,300]\n",
        "E = np.zeros((v_size+1, d_emb)) #300次元が縦に300万1個ある\n",
        "\n",
        "id_to_word = {0:\"<PAD>\"}\n",
        "word_to_id = {\"<PAD>\":0}\n",
        "\n",
        "for i,word in enumerate(model.key_to_index,1):\n",
        "  E[i] = model[word]  #model[word] : 単語をベクトルに変換する\n",
        "  id_to_word[i] = word  #{i:word}\n",
        "  word_to_id[word] = i  #{word:i}"
      ],
      "metadata": {
        "id": "oBNuvjyRMf3Z"
      },
      "id": "oBNuvjyRMf3Z",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
      "metadata": {
        "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
      },
      "source": [
        "## 71. データセットの読み込み\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
        "\n",
        "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
        "\n",
        "```\n",
        "{'text': 'contains no wit , only labored gags',\n",
        " 'label': tensor([0.]),\n",
        " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
        "```\n",
        "\n",
        "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxFowv2IAUDE",
        "outputId": "9c964f46-4e1b-4447-b394-a6e02f9e043c"
      },
      "id": "OxFowv2IAUDE",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-20 14:03:12--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.102, 108.157.254.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7439277 (7.1M) [application/zip]\n",
            "Saving to: ‘SST-2.zip’\n",
            "\n",
            "\rSST-2.zip             0%[                    ]       0  --.-KB/s               \rSST-2.zip           100%[===================>]   7.09M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-05-20 14:03:12 (109 MB/s) - ‘SST-2.zip’ saved [7439277/7439277]\n",
            "\n",
            "Archive:  SST-2.zip\n",
            "   creating: SST-2/\n",
            "  inflating: SST-2/dev.tsv           \n",
            "   creating: SST-2/original/\n",
            "  inflating: SST-2/original/README.txt  \n",
            "  inflating: SST-2/original/SOStr.txt  \n",
            "  inflating: SST-2/original/STree.txt  \n",
            "  inflating: SST-2/original/datasetSentences.txt  \n",
            "  inflating: SST-2/original/datasetSplit.txt  \n",
            "  inflating: SST-2/original/dictionary.txt  \n",
            "  inflating: SST-2/original/original_rt_snippets.txt  \n",
            "  inflating: SST-2/original/sentiment_labels.txt  \n",
            "  inflating: SST-2/test.tsv          \n",
            "  inflating: SST-2/train.tsv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
        "\n",
        "\n",
        "def word_to_id_change(text, word_to_id):\n",
        "  list = []\n",
        "  for word in text.split():\n",
        "    if word in word_to_id:\n",
        "      list.append(word_to_id[word])\n",
        "  return list\n",
        "\n",
        "def make_dataset(df, word_to_id):\n",
        "  dataset = []\n",
        "  for _,row in df.iterrows():\n",
        "    id = word_to_id_change(row[\"sentence\"], word_to_id)\n",
        "    if len(id) == 0:\n",
        "      continue\n",
        "    data = {\"text\": row[\"sentence\"], \"label\": torch.tensor([float(row[\"label\"])]), \"input_ids\": torch.tensor(id)}\n",
        "    dataset.append(data)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = make_dataset(train_df, word_to_id)\n",
        "dev_dataset = make_dataset(dev_df, word_to_id)"
      ],
      "metadata": {
        "id": "EaMvjEmbNAQG"
      },
      "id": "EaMvjEmbNAQG",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
      "metadata": {
        "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
      },
      "source": [
        "## 72. Bag of wordsモデルの構築\n",
        "\n",
        "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def average_bekutoru(ids, E):\n",
        "  sum = []\n",
        "  for i in ids:\n",
        "    sum.append(E[i.item()])\n",
        "  return torch.mean(torch.tensor(sum), dim=0).float()\n",
        "\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(embedding_dim, 1)  # w^T x + b\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)      # shape: (batch_size, 1)\n",
        "        out = self.sigmoid(out)   # shape: (batch_size, 1)\n",
        "        return out\n",
        "\n",
        "print(average_bekutoru(train_dataset[0][\"input_ids\"],E))"
      ],
      "metadata": {
        "id": "KCL3cjsTHc7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8eb41ef-97e2-4d19-a010-7bedbe7acfd6"
      },
      "id": "KCL3cjsTHc7s",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.5841, -0.1052, -0.0329, -0.1955,  0.1888,  0.4000, -0.1472, -0.2301,\n",
            "         0.2297, -0.1309,  0.3233,  0.1323,  0.2015, -0.1077, -0.0163,  0.1709,\n",
            "        -0.3846, -0.1186, -0.1999, -0.1891,  0.1664,  0.0469,  0.3069, -0.1310,\n",
            "        -0.4717, -1.0281, -0.1221, -0.0110,  0.2898, -0.2106,  2.6787,  0.1628,\n",
            "        -0.1114, -0.4462,  0.1151,  0.2763, -0.0297, -0.4203,  0.1303, -0.1559,\n",
            "         0.0170, -0.0847,  0.1870,  0.3271, -0.0883, -0.2434,  0.0543, -0.0810,\n",
            "        -0.1885, -0.2166])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-91d45d5908c9>:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  return torch.mean(torch.tensor(sum), dim=0).float()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
      "metadata": {
        "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
      },
      "source": [
        "## 73. モデルの学習\n",
        "\n",
        "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, dataset, E, epochs=5, batch_size=16, lr=0.001):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for i in range(0, len(dataset), batch_size):\n",
        "            batch = dataset[i:i+batch_size]\n",
        "\n",
        "            feature_vectors = []\n",
        "            labels = []\n",
        "\n",
        "            for example in batch:\n",
        "                avg_vec = average_bekutoru(example[\"input_ids\"], E)\n",
        "                feature_vectors.append(avg_vec)\n",
        "                labels.append(example[\"label\"].item())\n",
        "\n",
        "            inputs = torch.stack(feature_vectors)             # shape: (batch_size, embedding_dim)\n",
        "            targets = torch.tensor(labels, dtype=torch.float) # shape: (batch_size,)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()                 # shape: (batch_size,)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(dataset) // batch_size + 1)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# --------------------\n",
        "# 実行例（データとモデルの前提があるとする）\n",
        "# E: 事前学習済み埋め込み行列（torch.FloatTensor）\n",
        "# train_dataset: [{\"input_ids\": tensor([...]), \"label\": tensor([0.0 or 1.0])}, ...]\n",
        "\n",
        "embedding_dim = E.shape[1]\n",
        "model = SentimentClassifier(embedding_dim)\n",
        "\n",
        "# 学習実行\n",
        "train(model, train_dataset, E, epochs=10, batch_size=16)"
      ],
      "metadata": {
        "id": "uDcDhz8qTSKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac91b49-c148-42b5-c72b-7c6bf36162ba"
      },
      "id": "uDcDhz8qTSKz",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.5540\n",
            "Epoch 2/10 - Loss: 0.5094\n",
            "Epoch 3/10 - Loss: 0.5049\n",
            "Epoch 4/10 - Loss: 0.5039\n",
            "Epoch 5/10 - Loss: 0.5035\n",
            "Epoch 6/10 - Loss: 0.5034\n",
            "Epoch 7/10 - Loss: 0.5033\n",
            "Epoch 8/10 - Loss: 0.5033\n",
            "Epoch 9/10 - Loss: 0.5033\n",
            "Epoch 10/10 - Loss: 0.5033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
      "metadata": {
        "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
      },
      "source": [
        "## 74. モデルの評価\n",
        "\n",
        "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, E):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for example in dataset:\n",
        "            input_vec = average_bekutoru(example[\"input_ids\"], E).float().unsqueeze(0)  # shape: (1, embedding_dim)\n",
        "            output = model(input_vec)  # shape: (1, 1)\n",
        "            pred = 1 if output.item() >= 0.5 else 0\n",
        "            label = int(example[\"label\"].item())\n",
        "\n",
        "            if pred == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return print(f\"Accuracy on dev set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "evaluate(model, dev_dataset, E)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PAAL5O2742w",
        "outputId": "fb2f298b-2dc8-45f3-b28b-0e190fee7cc0"
      },
      "id": "3PAAL5O2742w",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on dev set: 70.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O08V9g0mcJwe",
      "metadata": {
        "id": "O08V9g0mcJwe"
      },
      "source": [
        "## 75. パディング\n",
        "\n",
        "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
        "\n",
        "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
        "\n",
        "```\n",
        "[{'text': 'hide new secretions from the parental units',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
        " {'text': 'contains no wit , only labored gags',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
        " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
        "  'label': tensor([1.]),\n",
        "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
        " {'text': 'remains utterly satisfied to remain the same throughout',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
        "```\n",
        "\n",
        "`collate`関数を通した結果は以下のようになることが想定される。\n",
        "\n",
        "```\n",
        "{'input_ids': tensor([\n",
        "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
        "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
        "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
        "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
        " 'label': tensor([\n",
        "    [1.],\n",
        "    [0.],\n",
        "    [0.],\n",
        "    [0.]])}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch):\n",
        "    # 各入力系列の長さを取得\n",
        "    lengths = [len(example['input_ids']) for example in batch]\n",
        "\n",
        "    # 長さの降順にソート\n",
        "    sorted_batch = sorted(batch, key=lambda x: len(x['input_ids']), reverse=True)\n",
        "\n",
        "    # 最大長を取得\n",
        "    max_len = len(sorted_batch[0]['input_ids'])\n",
        "\n",
        "    # パディングとテンソル化\n",
        "    padded_input_ids = []\n",
        "    labels = []\n",
        "\n",
        "    for example in sorted_batch:\n",
        "        ids = example['input_ids']\n",
        "        padded = torch.cat([ids, torch.zeros(max_len - len(ids), dtype=torch.long)])\n",
        "        padded_input_ids.append(padded)\n",
        "        labels.append(example['label'])\n",
        "\n",
        "    # まとめて1つのテンソルに\n",
        "    input_ids_tensor = torch.stack(padded_input_ids)\n",
        "    labels_tensor = torch.stack(labels)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids_tensor,\n",
        "        'label': labels_tensor\n",
        "    }\n",
        "\n",
        "\n",
        "batch = [train_dataset[i] for i in range(4)]\n",
        "batch_tensor = collate(batch)\n",
        "print(batch_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YYHaUTe8SAr",
        "outputId": "3821e5c5-5609-4df7-d7b8-f9be1e300f35"
      },
      "id": "8YYHaUTe8SAr",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   13,  6742,    48,  2154,     6, 36257,   646,   872,  3367,    60,\n",
            "           474,  1747],\n",
            "        [  949, 14306,  5457,     5,   945,     1,   216,   984,     0,     0,\n",
            "             0,     0],\n",
            "        [ 5709,    51, 52777,    26,     1, 13055,  1504,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 2434,    85, 13026,     2,    92, 26399, 31352,     0,     0,     0,\n",
            "             0,     0]]), 'label': tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NzvuZ-5ebDU",
      "metadata": {
        "id": "9NzvuZ-5ebDU"
      },
      "source": [
        "## 76. ミニバッチ学習\n",
        "\n",
        "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_average_bekutoru(batch_input_ids, E):\n",
        "    vectors = []\n",
        "    for input_ids in batch_input_ids:\n",
        "        vecs = [torch.tensor(E[i], dtype=torch.float32) for i in input_ids if i != 0]\n",
        "        if len(vecs) == 0:\n",
        "            # 万一全て0だった場合（安全対策）\n",
        "            vecs = [torch.zeros(E.shape[1])]\n",
        "        avg = torch.mean(torch.stack(vecs), dim=0)\n",
        "        vectors.append(avg)\n",
        "    return torch.stack(vectors)"
      ],
      "metadata": {
        "id": "GlzB0kxu-nBQ"
      },
      "id": "GlzB0kxu-nBQ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, E, train_dataset, batch_size=16, epochs=10):\n",
        "    model.train()\n",
        "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "          optimizer.zero_grad()\n",
        "          batch_input_ids = batch['input_ids']\n",
        "          batch_vectors = batch_average_bekutoru(batch_input_ids, E)\n",
        "\n",
        "          labels = batch['label'].float()  # ラベルを float に\n",
        "          outputs = model(batch_vectors).float()  # 出力を float に\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "T_qaE4np-uxN"
      },
      "id": "T_qaE4np-uxN",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, E, batch_size=16):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            labels = batch['label']\n",
        "\n",
        "            features = batch_average_bekutoru(input_ids, E).float()\n",
        "            outputs = model(features)\n",
        "            preds = (outputs >= 0.5).float()\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "jiXLJCiR-z_t"
      },
      "id": "jiXLJCiR-z_t",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = E.shape[1]\n",
        "model = SentimentClassifier(embedding_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# 学習\n",
        "train_model(model, E, train_dataset, batch_size=16, epochs=10)\n",
        "\n",
        "# 評価\n",
        "evaluate(model, dev_dataset, E)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpXLjxu9_Jeh",
        "outputId": "8d3c0088-3a24-466a-9bba-7c05281f4c78"
      },
      "id": "JpXLjxu9_Jeh",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2313.9349\n",
            "Epoch 2, Loss: 2133.0649\n",
            "Epoch 3, Loss: 2114.3246\n",
            "Epoch 4, Loss: 2110.0443\n",
            "Epoch 5, Loss: 2108.2490\n",
            "Epoch 6, Loss: 2107.9252\n",
            "Epoch 7, Loss: 2107.2241\n",
            "Epoch 8, Loss: 2107.2294\n",
            "Epoch 9, Loss: 2107.3685\n",
            "Epoch 10, Loss: 2107.0319\n",
            "Accuracy: 70.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RUbjivUTejxn",
      "metadata": {
        "id": "RUbjivUTejxn"
      },
      "source": [
        "## 77. GPU上での学習\n",
        "\n",
        "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUY1PsD-eplq",
      "metadata": {
        "id": "ZUY1PsD-eplq"
      },
      "source": [
        "## 78. 単語埋め込みのファインチューニング\n",
        "\n",
        "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jVAdWIq0evKR",
      "metadata": {
        "id": "jVAdWIq0evKR"
      },
      "source": [
        "## 79. アーキテクチャの変更\n",
        "\n",
        "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}