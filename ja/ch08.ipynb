{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e1YwuFtZd1t",
      "metadata": {
        "editable": true,
        "id": "1e1YwuFtZd1t",
        "tags": []
      },
      "source": [
        "# 第8章: ニューラルネット\n",
        "\n",
        "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "import numpy\n",
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "snDcKzyym8CD",
        "outputId": "ec089391-1c63-4c80-c2d3-8af125723c52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "snDcKzyym8CD",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
      "metadata": {
        "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
      },
      "source": [
        "## 70. 単語埋め込みの読み込み\n",
        "\n",
        "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
        "\n",
        "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
        "\n",
        "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "v_size = len(model.key_to_index)  #model.key_to_index : 単語を数字に変換するための辞書   v_size : 300万単語\n",
        "d_emb = model.vector_size #d_emb : 300次元 [1,2,3,...,300]\n",
        "E = np.zeros((v_size+1, d_emb)) #300次元が縦に300万1個ある\n",
        "\n",
        "id_to_word = {0:\"<PAD>\"}\n",
        "word_to_id = {\"<PAD>\":0}\n",
        "\n",
        "for i,word in enumerate(model.key_to_index,1):\n",
        "  E[i] = model[word]  #model[word] : 単語をベクトルに変換する\n",
        "  id_to_word[i] = word  #{i:word}\n",
        "  word_to_id[word] = i  #{word:i}\n",
        "\n",
        "print(E[0])\n",
        "print(E[1])"
      ],
      "metadata": {
        "id": "oBNuvjyRMf3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0e37fb-0252-4dff-8cae-4a45665b2717"
      },
      "id": "oBNuvjyRMf3Z",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[ 1.12915039e-03 -8.96453857e-04  3.18527222e-04  1.53350830e-03\n",
            "  1.10626221e-03 -1.40380859e-03 -3.05175781e-05 -4.19616699e-04\n",
            " -5.76019287e-04  1.07574463e-03 -1.02233887e-03 -6.17980957e-04\n",
            " -7.55310059e-04  1.40380859e-03 -1.64031982e-03 -6.33239746e-04\n",
            "  1.63269043e-03 -1.00708008e-03 -1.26647949e-03  6.52313232e-04\n",
            " -4.15802002e-04 -1.07574463e-03  1.52587891e-03 -2.74658203e-04\n",
            "  1.40190125e-04  1.57165527e-03  1.35803223e-03 -8.31604004e-04\n",
            " -1.40380859e-03  1.57928467e-03  2.53677368e-04 -7.32421875e-04\n",
            " -1.05381012e-04 -1.16729736e-03  1.57928467e-03  6.56127930e-04\n",
            " -6.59942627e-04  2.92062759e-06  1.12915039e-03  4.27246094e-04\n",
            " -3.70025635e-04 -1.15203857e-03  1.26647949e-03 -3.51667404e-06\n",
            "  2.65121460e-04 -4.02450562e-04  1.41143799e-04 -3.36170197e-05\n",
            "  7.59124756e-04 -5.18798828e-04 -7.10487366e-05  6.02722168e-04\n",
            " -5.07354736e-04 -1.62506104e-03 -4.36782837e-04 -9.91821289e-04\n",
            " -1.22070312e-03 -3.22341919e-04  6.86645508e-05 -1.16729736e-03\n",
            " -5.11169434e-04  1.41143799e-03  3.35693359e-04 -4.74929810e-04\n",
            " -1.37329102e-03  3.66210938e-04 -1.44195557e-03 -6.06536865e-04\n",
            "  8.01086426e-04  1.12915039e-03 -8.35418701e-04 -1.15966797e-03\n",
            "  9.15527344e-04  5.22613525e-04 -3.28063965e-04  1.59454346e-03\n",
            " -1.57928467e-03 -3.56674194e-04  4.95910645e-04  1.01470947e-03\n",
            " -1.09863281e-03 -1.65939331e-04 -1.42097473e-04 -2.61306763e-04\n",
            "  1.25885010e-03  3.86238098e-05  1.68800354e-04 -1.02996826e-03\n",
            "  1.60980225e-03  6.29425049e-04  4.17709351e-04 -1.35040283e-03\n",
            "  3.49044800e-04  1.14440918e-03 -1.20544434e-03 -1.18255615e-03\n",
            "  9.49859619e-04  6.05583191e-05  1.07288361e-05 -6.67572021e-04\n",
            "  1.24359131e-03  6.90460205e-04  5.55515289e-05 -8.62121582e-04\n",
            " -1.16729736e-03  1.21307373e-03 -8.04901123e-04 -8.77380371e-04\n",
            "  2.27928162e-04 -3.96728516e-04 -8.58306885e-04  2.88009644e-04\n",
            " -1.58691406e-03  4.84466553e-04 -1.12152100e-03  1.96695328e-06\n",
            " -3.79562378e-04  7.05718994e-04 -1.58691406e-03  1.62506104e-03\n",
            "  1.55639648e-03 -4.31060791e-04  9.84191895e-04  9.04083252e-04\n",
            " -1.39617920e-03  1.20544434e-03 -7.01904297e-04  2.70843506e-04\n",
            " -1.23596191e-03  6.90460205e-04 -8.43048096e-04  1.34277344e-03\n",
            " -1.43432617e-03 -6.71386719e-04  1.54876709e-03 -1.09863281e-03\n",
            "  1.19018555e-03 -1.42669678e-03 -6.82830811e-04 -7.85827637e-04\n",
            "  4.80651855e-04  4.08172607e-04 -6.37054443e-04  1.44958496e-04\n",
            " -9.76562500e-04  1.44958496e-03  8.45640898e-07 -1.66320801e-03\n",
            " -3.28063965e-04  6.29425049e-04 -1.43432617e-03 -3.41415405e-04\n",
            "  1.15203857e-03 -5.30242920e-04 -4.71115112e-04 -8.50677490e-04\n",
            " -1.38092041e-03 -1.24359131e-03 -1.32751465e-03  1.07574463e-03\n",
            "  1.31988525e-03 -3.03268433e-04 -3.74317169e-05  1.18255615e-03\n",
            " -1.35803223e-03 -1.04522705e-03  5.67436218e-05 -1.01470947e-03\n",
            "  4.32968140e-04 -1.57165527e-03 -9.10758972e-05  1.06048584e-03\n",
            " -6.02722168e-04 -1.53350830e-03 -1.53350830e-03  5.41687012e-04\n",
            "  1.33514404e-03  4.11987305e-04 -3.10897827e-04  1.76429749e-04\n",
            " -1.37329102e-04 -6.98089600e-04 -8.62121582e-04 -1.08337402e-03\n",
            " -2.98023224e-05  8.01086426e-04  6.79016113e-04  3.35693359e-04\n",
            " -1.38854980e-03  1.35040283e-03  2.34603882e-04 -1.33514404e-03\n",
            " -8.73565674e-04 -7.43865967e-04  1.08337402e-03  6.05583191e-05\n",
            " -1.26647949e-03  1.19018555e-03 -6.21795654e-04  1.35973096e-07\n",
            "  1.27410889e-03 -9.84191895e-04 -1.54876709e-03  1.55639648e-03\n",
            " -1.31225586e-03 -7.93457031e-04  1.53350830e-03  1.29699707e-03\n",
            " -1.80244446e-04  9.19342041e-04  1.20544434e-03  7.70568848e-04\n",
            " -1.65557861e-03  7.70568848e-04  1.44958496e-03 -1.30462646e-03\n",
            "  6.10351562e-04  6.59942627e-04  1.25885010e-03  1.41906738e-03\n",
            " -1.22070312e-03 -1.51062012e-03  1.12915039e-03  1.34277344e-03\n",
            "  1.66320801e-03 -5.72204590e-04 -5.56945801e-04  3.98635864e-04\n",
            " -2.70843506e-04  4.95910645e-04  1.60980225e-03 -7.05718994e-04\n",
            "  6.25610352e-04 -9.76562500e-04 -1.89781189e-04  9.53674316e-05\n",
            " -5.18798828e-04 -2.04086304e-04 -8.27789307e-04 -1.23023987e-04\n",
            "  7.62939453e-04  3.22341919e-04 -1.24359131e-03  9.91821289e-04\n",
            "  1.06048584e-03 -1.41143799e-03  9.67979431e-05 -1.55639648e-03\n",
            "  2.19345093e-04 -5.53131104e-05 -9.11712646e-04 -1.48773193e-03\n",
            "  1.36566162e-03 -8.43048096e-04 -4.19616699e-04  3.24249268e-04\n",
            " -1.00708008e-03  1.25885010e-04 -4.55856323e-04  1.92642212e-04\n",
            " -2.68936157e-04  1.49536133e-03 -1.58691406e-03  5.91278076e-04\n",
            " -1.46484375e-03  9.65118408e-04 -1.28173828e-03  1.60217285e-03\n",
            "  1.09100342e-03 -1.31225586e-03  1.09100342e-03 -5.11169434e-04\n",
            "  3.45230103e-04  1.04522705e-03 -2.06947327e-04  9.04083252e-04\n",
            "  6.67572021e-04  1.10626221e-03 -8.73565674e-04 -3.75747681e-04\n",
            " -2.57492065e-04 -9.15527344e-05  1.43432617e-03 -1.18255615e-03\n",
            " -8.72612000e-05  1.32751465e-03 -1.58309937e-04  1.28936768e-03\n",
            " -9.84191895e-04 -5.49316406e-04 -1.54876709e-03  1.37329102e-03\n",
            " -6.07967377e-05 -8.23974609e-04  1.32751465e-03  1.15966797e-03\n",
            "  5.68389893e-04 -1.56402588e-03 -1.23023987e-04 -8.63075256e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
      "metadata": {
        "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
      },
      "source": [
        "## 71. データセットの読み込み\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
        "\n",
        "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
        "\n",
        "```\n",
        "{'text': 'contains no wit , only labored gags',\n",
        " 'label': tensor([0.]),\n",
        " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
        "```\n",
        "\n",
        "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")"
      ],
      "metadata": {
        "id": "EaMvjEmbNAQG"
      },
      "id": "EaMvjEmbNAQG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
      "metadata": {
        "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
      },
      "source": [
        "## 72. Bag of wordsモデルの構築\n",
        "\n",
        "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
      "metadata": {
        "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
      },
      "source": [
        "## 73. モデルの学習\n",
        "\n",
        "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
      "metadata": {
        "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
      },
      "source": [
        "## 74. モデルの評価\n",
        "\n",
        "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O08V9g0mcJwe",
      "metadata": {
        "id": "O08V9g0mcJwe"
      },
      "source": [
        "## 75. パディング\n",
        "\n",
        "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
        "\n",
        "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
        "\n",
        "```\n",
        "[{'text': 'hide new secretions from the parental units',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
        " {'text': 'contains no wit , only labored gags',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
        " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
        "  'label': tensor([1.]),\n",
        "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
        " {'text': 'remains utterly satisfied to remain the same throughout',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
        "```\n",
        "\n",
        "`collate`関数を通した結果は以下のようになることが想定される。\n",
        "\n",
        "```\n",
        "{'input_ids': tensor([\n",
        "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
        "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
        "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
        "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
        " 'label': tensor([\n",
        "    [1.],\n",
        "    [0.],\n",
        "    [0.],\n",
        "    [0.]])}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NzvuZ-5ebDU",
      "metadata": {
        "id": "9NzvuZ-5ebDU"
      },
      "source": [
        "## 76. ミニバッチ学習\n",
        "\n",
        "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RUbjivUTejxn",
      "metadata": {
        "id": "RUbjivUTejxn"
      },
      "source": [
        "## 77. GPU上での学習\n",
        "\n",
        "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUY1PsD-eplq",
      "metadata": {
        "id": "ZUY1PsD-eplq"
      },
      "source": [
        "## 78. 単語埋め込みのファインチューニング\n",
        "\n",
        "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jVAdWIq0evKR",
      "metadata": {
        "id": "jVAdWIq0evKR"
      },
      "source": [
        "## 79. アーキテクチャの変更\n",
        "\n",
        "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}