{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "1HhY1_KNjX_Y",
        "tags": []
      },
      "source": [
        "# 第9章: 事前学習済み言語モデル（BERT型）\n",
        "\n",
        "本章では、BERT型の事前学習済みモデルを利用して、マスク単語の予測や文ベクトルの計算、評判分析器（ポジネガ分類器）の構築に取り組む。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyLKl_eo_80z"
      },
      "source": [
        "## 80. トークン化\n",
        "\n",
        "\"The movie was full of incomprehensibilities.\"という文をトークンに分解し、トークン列を表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging, BertTokenizer\n",
        "\n",
        "# 警告を抑制（重要なエラーだけ表示）\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = \"The movie was full of incomprehensibilities.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "MUtCt8K5F9sW",
        "outputId": "7acfc5c4-57be-46e6-d249-0d112a60f9a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'movie', 'was', 'full', 'of', 'inc', '##omp', '##re', '##hen', '##si', '##bilities', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lbx12UDACt6"
      },
      "source": [
        "## 81. マスクの予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"を埋めるのに最も適切なトークンを求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from pprint import pprint\n",
        "\n",
        "# パイプライン作成と予測\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "results = unmasker(\"The movie was full of [MASK].\")\n",
        "pprint(results[0])\n"
      ],
      "metadata": {
        "id": "S19co1BfGpzW",
        "outputId": "204080f7-8cdf-467b-fff6-fc2e5a3c3ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.10711909830570221,\n",
            " 'sequence': 'the movie was full of fun.',\n",
            " 'token': 4569,\n",
            " 'token_str': 'fun'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F39DStXDk3OG"
      },
      "source": [
        "## 82. マスクのtop-k予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"に埋めるのに適切なトークン上位10個と、その確率（尤度）を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipelineを作成（マスク補完用）\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# 入力文（[MASK]は必ず大文字で）\n",
        "text = \"The movie was full of [MASK].\"\n",
        "\n",
        "# top_k=10で上位10個の予測を取得\n",
        "results = unmasker(text, top_k=10)\n",
        "\n",
        "# 結果の表示\n",
        "for i, result in enumerate(results, 1):\n",
        "    token = result[\"token_str\"]\n",
        "    score = result[\"score\"]\n",
        "    print(f\"{i}. {token:<15} (probability: {score:.4f})\")\n"
      ],
      "metadata": {
        "id": "yqVDW-wwIvwX",
        "outputId": "a2c7d7c9-2d86-4765-ef22-dc8b25eab474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. fun             (probability: 0.1071)\n",
            "2. surprises       (probability: 0.0663)\n",
            "3. drama           (probability: 0.0447)\n",
            "4. stars           (probability: 0.0272)\n",
            "5. laughs          (probability: 0.0254)\n",
            "6. action          (probability: 0.0195)\n",
            "7. excitement      (probability: 0.0190)\n",
            "8. people          (probability: 0.0183)\n",
            "9. tension         (probability: 0.0150)\n",
            "10. music           (probability: 0.0146)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zr6VVYiRPzk"
      },
      "source": [
        "## 83. CLSトークンによる文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import itertools\n",
        "\n",
        "# デバイス設定（GPUがあるなら使う）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# モデルとトークナイザのロード\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 対象の文\n",
        "sentences = [\n",
        "    \"The movie was full of fun.\",\n",
        "    \"The movie was full of excitement.\",\n",
        "    \"The movie was full of crap.\",\n",
        "    \"The movie was full of rubbish.\"\n",
        "]\n",
        "\n",
        "# 各文に対する [CLS] トークンの最終層埋め込みを取得\n",
        "cls_embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        outputs = model(**inputs)\n",
        "        cls_embed = outputs.last_hidden_state[:, 0, :]  # [CLS] トークンは位置 0\n",
        "        cls_embeddings.append(cls_embed.cpu())\n",
        "\n",
        "# 埋め込みを1つのテンソルにまとめて numpy 配列に変換\n",
        "cls_embeddings = torch.cat(cls_embeddings, dim=0).numpy()\n",
        "\n",
        "# コサイン類似度の計算\n",
        "similarities = cosine_similarity(cls_embeddings)\n",
        "\n",
        "# 出力：すべてのペアのコサイン類似度\n",
        "pairs = list(itertools.combinations(range(len(sentences)), 2))\n",
        "for i, j in pairs:\n",
        "    print(f\"Similarity between:\\n  \\\"{sentences[i]}\\\"\\n  \\\"{sentences[j]}\\\"\\n  => {similarities[i][j]:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkxXqePKxVgS",
        "outputId": "e780b429-bbd0-4604-889e-064f3c8e0615"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of excitement.\"\n",
            "  => 0.9881\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.9558\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9475\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.9541\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9487\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of crap.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9807\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqo5Ufzkyc89"
      },
      "source": [
        "## 84. 平均による文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import itertools\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# モデルとトークナイザのロード\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 対象文\n",
        "sentences = [\n",
        "    \"The movie was full of fun.\",\n",
        "    \"The movie was full of excitement.\",\n",
        "    \"The movie was full of crap.\",\n",
        "    \"The movie was full of rubbish.\"\n",
        "]\n",
        "\n",
        "# 各文に対して平均埋め込みを計算\n",
        "mean_embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # last_hidden_state: (1, seq_len, hidden_dim)\n",
        "        token_embeddings = outputs.last_hidden_state.squeeze(0)  # (seq_len, hidden_dim)\n",
        "        attention_mask = inputs[\"attention_mask\"].squeeze(0)     # (seq_len)\n",
        "\n",
        "        # attention_mask を使って、PAD トークンを除外して平均\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
        "        sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=0)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(dim=0), min=1e-9)\n",
        "        mean_embedding = sum_embeddings / sum_mask\n",
        "\n",
        "        mean_embeddings.append(mean_embedding.cpu())\n",
        "\n",
        "# numpy に変換してコサイン類似度を計算\n",
        "mean_embeddings = torch.stack(mean_embeddings).numpy()\n",
        "similarities = cosine_similarity(mean_embeddings)\n",
        "\n",
        "# 出力\n",
        "pairs = list(itertools.combinations(range(len(sentences)), 2))\n",
        "for i, j in pairs:\n",
        "    print(f\"Similarity between:\\n  \\\"{sentences[i]}\\\"\\n  \\\"{sentences[j]}\\\"\\n  => {similarities[i][j]:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJqVi0SZx4QO",
        "outputId": "717fbf61-57ff-4beb-a9ff-5d8dbfb4ae2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of excitement.\"\n",
            "  => 0.9568\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.8490\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.8169\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.8352\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.7938\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of crap.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9226\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s934DWT_1kFm"
      },
      "source": [
        "## 85. データセットの準備\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) から訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、さらに全てのテキストはトークン列に変換せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kSd-ijqyCrV",
        "outputId": "51a956b0-1da3-488a-a172-2fd4bf1bbfb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-23 04:43:12--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.15, 108.157.254.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7439277 (7.1M) [application/zip]\n",
            "Saving to: ‘SST-2.zip’\n",
            "\n",
            "SST-2.zip           100%[===================>]   7.09M  5.53MB/s    in 1.3s    \n",
            "\n",
            "2025-05-23 04:43:14 (5.53 MB/s) - ‘SST-2.zip’ saved [7439277/7439277]\n",
            "\n",
            "Archive:  SST-2.zip\n",
            "   creating: SST-2/\n",
            "  inflating: SST-2/dev.tsv           \n",
            "   creating: SST-2/original/\n",
            "  inflating: SST-2/original/README.txt  \n",
            "  inflating: SST-2/original/SOStr.txt  \n",
            "  inflating: SST-2/original/STree.txt  \n",
            "  inflating: SST-2/original/datasetSentences.txt  \n",
            "  inflating: SST-2/original/datasetSplit.txt  \n",
            "  inflating: SST-2/original/dictionary.txt  \n",
            "  inflating: SST-2/original/original_rt_snippets.txt  \n",
            "  inflating: SST-2/original/sentiment_labels.txt  \n",
            "  inflating: SST-2/test.tsv          \n",
            "  inflating: SST-2/train.tsv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('SST-2/train.tsv', sep='\\t')\n",
        "dev_data = pd.read_csv('SST-2/dev.tsv', sep='\\t')\n",
        "\n",
        "train_data1 = []\n",
        "for _,j in train_data.iterrows():\n",
        "  tokens = tokenizer.tokenize(j[\"sentence\"])\n",
        "  data = {\"sentence\":tokens,\"label\":j[\"label\"]}\n",
        "  train_data1.append(data)\n",
        "\n",
        "dev_data1 = []\n",
        "for _,j in dev_data.iterrows():\n",
        "  tokens = tokenizer.tokenize(j[\"sentence\"])\n",
        "  data = {\"sentence\":tokens,\"label\":j[\"label\"]}\n",
        "  dev_data1.append(data)"
      ],
      "metadata": {
        "id": "7WAWJHpOyIzB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiXmLuWpD3_Q"
      },
      "source": [
        "## 86. ミニバッチの作成\n",
        "\n",
        "85で読み込んだ訓練データの一部（例えば冒頭の4事例）に対して、パディングなどの処理を行い、トークン列の長さを揃えてミニバッチを構成せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 86の解答：冒頭4事例に対してパディングを行い、ミニバッチを構成する\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# トークナイザーの準備（BERTを例とする）\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 冒頭の4事例だけを対象にする\n",
        "train_data_subset = train_data1[:4]\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "\n",
        "for i in train_data_subset:\n",
        "    # トークナイズしてID化、特殊トークン追加、attention mask生成\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        \" \".join(i['sentence']),\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids.append(encoded['input_ids'].squeeze(0))\n",
        "    attention_masks.append(encoded['attention_mask'].squeeze(0))\n",
        "    labels.append(int(i['label']))\n",
        "\n",
        "# パディングしてバッチ化（最大長に揃える）\n",
        "input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Input IDs (padded):\")\n",
        "print(input_ids_padded)\n",
        "print(\"\\nAttention Masks:\")\n",
        "print(attention_masks_padded)\n",
        "print(\"\\nLabels:\")\n",
        "print(labels_tensor)\n"
      ],
      "metadata": {
        "id": "OUdgXTjy0er3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fe9b5c-03a1-44dc-a969-c39f385c9029"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs (padded):\n",
            "tensor([[  101,  5342,  2047,  3595,  1001,  1001, 15956,  2013,  1996, 18643,\n",
            "          3197,   102,     0,     0,     0,     0,     0],\n",
            "        [  101,  3397,  2053, 15966,  1010,  2069,  4450,  1001,  1001,  3968,\n",
            "         18201,  1001,  1001,  1055,   102,     0,     0],\n",
            "        [  101,  2008,  7459,  2049,  3494,  1998, 10639,  1001,  1001,  1055,\n",
            "          2242,  2738,  3376,  2055,  2529,  3267,   102],\n",
            "        [  101,  3464, 12580,  8510,  2000,  3961,  1996,  2168,  2802,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "Attention Masks:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
            "\n",
            "Labels:\n",
            "tensor([0, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4nrV6bqD-m9"
      },
      "source": [
        "## 87. ファインチューニング\n",
        "\n",
        "訓練セットを用い、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 軽量モデルとトークナイザー\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "class SST2Dataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.encodings = tokenizer(\n",
        "            df[\"sentence\"].tolist(),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.labels = torch.tensor(df[\"label\"].tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()} | {\"labels\": self.labels[idx]}\n",
        "\n",
        "train_dataset = SST2Dataset(train_data, tokenizer)\n",
        "dev_dataset = SST2Dataset(dev_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16, num_workers=0)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "epochs = 1  # エポック数10に変更\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            targets.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    print(f\"Epoch {epoch+1} Validation Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "ml9z8G5r5TTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d55a27-8d72-4163-dc1f-7ea848ebb76c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 4210/4210 [11:51<00:00,  5.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Loss: 0.2146\n",
            "Epoch 1 Validation Accuracy: 0.8979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OqDN0G2UqFJ"
      },
      "source": [
        "## 88. 極性分析\n",
        "\n",
        "問題87でファインチューニングされたモデルを用いて、以下の文の極性を予測せよ。\n",
        "\n",
        "- \"The movie was full of incomprehensibilities.\"\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論対象の文\n",
        "sentences = [\n",
        "    \"The movie was full of incomprehensibilities.\",\n",
        "    \"The movie was full of fun.\",\n",
        "    \"The movie was full of excitement.\",\n",
        "    \"The movie was full of crap.\",\n",
        "    \"The movie was full of rubbish.\"\n",
        "]\n",
        "\n",
        "# モデルを評価モードに\n",
        "model.eval()\n",
        "\n",
        "# 各文に対して推論\n",
        "for sentence in sentences:\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class_id = logits.argmax(dim=1).item()\n",
        "\n",
        "    label_str = \"positive\" if predicted_class_id == 1 else \"negative\"\n",
        "    print(f\"\\\"{sentence}\\\" => {label_str}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQD92zOne1v",
        "outputId": "cb049ed0-5523-46c9-f039-551a7821d59b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"The movie was full of incomprehensibilities.\" => negative\n",
            "\"The movie was full of fun.\" => positive\n",
            "\"The movie was full of excitement.\" => positive\n",
            "\"The movie was full of crap.\" => negative\n",
            "\"The movie was full of rubbish.\" => negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EruVW7JaapIv"
      },
      "source": [
        "## 89. アーキテクチャの変更\n",
        "\n",
        "問題87とは異なるアーキテクチャ（例えば[CLS]トークンを用いるか、各トークンの最大値プーリングを用いるなど）の分類モデルを設計し、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# DistilBERT 本体 + 自作の出力層に置き換え（CLSトークン明示使用）\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "class DistilBertClassifier(nn.Module):\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=2):\n",
        "        super().__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0]  # CLSトークン\n",
        "        logits = self.classifier(cls_output)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {'loss': loss, 'logits': logits}\n"
      ],
      "metadata": {
        "id": "W-XZd0HcoHCW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル初期化\n",
        "model = DistilBertClassifier().to(device)\n",
        "\n",
        "# 残りは以前のコードと同様：\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs['loss']\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # 評価\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs[\"logits\"]\n",
        "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            targets.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    print(f\"Epoch {epoch+1} Validation Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRUM5kgUoJLH",
        "outputId": "23ebbe77-0afb-462e-c11e-9798d1075841"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 4210/4210 [11:50<00:00,  5.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Loss: 0.2109\n",
            "Epoch 1 Validation Accuracy: 0.9002\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}