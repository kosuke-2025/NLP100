{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "1HhY1_KNjX_Y",
        "tags": []
      },
      "source": [
        "# 第9章: 事前学習済み言語モデル（BERT型）\n",
        "\n",
        "本章では、BERT型の事前学習済みモデルを利用して、マスク単語の予測や文ベクトルの計算、評判分析器（ポジネガ分類器）の構築に取り組む。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyLKl_eo_80z"
      },
      "source": [
        "## 80. トークン化\n",
        "\n",
        "\"The movie was full of incomprehensibilities.\"という文をトークンに分解し、トークン列を表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging, BertTokenizer\n",
        "\n",
        "# 警告を抑制（重要なエラーだけ表示）\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = \"The movie was full of incomprehensibilities.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "MUtCt8K5F9sW",
        "outputId": "7c8f049d-66e6-4520-9df9-bf0ac1bfc710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'movie', 'was', 'full', 'of', 'inc', '##omp', '##re', '##hen', '##si', '##bilities', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lbx12UDACt6"
      },
      "source": [
        "## 81. マスクの予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"を埋めるのに最も適切なトークンを求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from pprint import pprint\n",
        "\n",
        "# パイプライン作成と予測\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "results = unmasker(\"The movie was full of [MASK].\")\n",
        "pprint(results[0])\n"
      ],
      "metadata": {
        "id": "S19co1BfGpzW",
        "outputId": "a43c965d-7817-4b7a-9694-92bc692a3652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.10711909830570221,\n",
            " 'sequence': 'the movie was full of fun.',\n",
            " 'token': 4569,\n",
            " 'token_str': 'fun'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F39DStXDk3OG"
      },
      "source": [
        "## 82. マスクのtop-k予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"に埋めるのに適切なトークン上位10個と、その確率（尤度）を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipelineを作成（マスク補完用）\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# 入力文（[MASK]は必ず大文字で）\n",
        "text = \"The movie was full of [MASK].\"\n",
        "\n",
        "# top_k=10で上位10個の予測を取得\n",
        "results = unmasker(text, top_k=10)\n",
        "\n",
        "# 結果の表示\n",
        "for i, result in enumerate(results, 1):\n",
        "    token = result[\"token_str\"]\n",
        "    score = result[\"score\"]\n",
        "    print(f\"{i}. {token:<15} (probability: {score:.4f})\")\n"
      ],
      "metadata": {
        "id": "yqVDW-wwIvwX",
        "outputId": "23747f8f-2d23-41e1-ad10-854672801844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. fun             (probability: 0.1071)\n",
            "2. surprises       (probability: 0.0663)\n",
            "3. drama           (probability: 0.0447)\n",
            "4. stars           (probability: 0.0272)\n",
            "5. laughs          (probability: 0.0254)\n",
            "6. action          (probability: 0.0195)\n",
            "7. excitement      (probability: 0.0190)\n",
            "8. people          (probability: 0.0183)\n",
            "9. tension         (probability: 0.0150)\n",
            "10. music           (probability: 0.0146)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zr6VVYiRPzk"
      },
      "source": [
        "## 83. CLSトークンによる文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import itertools\n",
        "\n",
        "# デバイス設定（GPUがあるなら使う）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# モデルとトークナイザのロード\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 対象の文\n",
        "sentences = [\n",
        "    \"The movie was full of fun.\",\n",
        "    \"The movie was full of excitement.\",\n",
        "    \"The movie was full of crap.\",\n",
        "    \"The movie was full of rubbish.\"\n",
        "]\n",
        "\n",
        "# 各文に対する [CLS] トークンの最終層埋め込みを取得\n",
        "cls_embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        outputs = model(**inputs)\n",
        "        cls_embed = outputs.last_hidden_state[:, 0, :]  # [CLS] トークンは位置 0\n",
        "        cls_embeddings.append(cls_embed.cpu())\n",
        "\n",
        "# 埋め込みを1つのテンソルにまとめて numpy 配列に変換\n",
        "cls_embeddings = torch.cat(cls_embeddings, dim=0).numpy()\n",
        "\n",
        "# コサイン類似度の計算\n",
        "similarities = cosine_similarity(cls_embeddings)\n",
        "\n",
        "# 出力：すべてのペアのコサイン類似度\n",
        "pairs = list(itertools.combinations(range(len(sentences)), 2))\n",
        "for i, j in pairs:\n",
        "    print(f\"Similarity between:\\n  \\\"{sentences[i]}\\\"\\n  \\\"{sentences[j]}\\\"\\n  => {similarities[i][j]:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkxXqePKxVgS",
        "outputId": "4cc08a14-854f-49c1-f0de-18dc72e00202"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of excitement.\"\n",
            "  => 0.9881\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.9558\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9475\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.9541\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9487\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of crap.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9807\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqo5Ufzkyc89"
      },
      "source": [
        "## 84. 平均による文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import itertools\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# モデルとトークナイザのロード\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 対象文\n",
        "sentences = [\n",
        "    \"The movie was full of fun.\",\n",
        "    \"The movie was full of excitement.\",\n",
        "    \"The movie was full of crap.\",\n",
        "    \"The movie was full of rubbish.\"\n",
        "]\n",
        "\n",
        "# 各文に対して平均埋め込みを計算\n",
        "mean_embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # last_hidden_state: (1, seq_len, hidden_dim)\n",
        "        token_embeddings = outputs.last_hidden_state.squeeze(0)  # (seq_len, hidden_dim)\n",
        "        attention_mask = inputs[\"attention_mask\"].squeeze(0)     # (seq_len)\n",
        "\n",
        "        # attention_mask を使って、PAD トークンを除外して平均\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
        "        sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=0)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(dim=0), min=1e-9)\n",
        "        mean_embedding = sum_embeddings / sum_mask\n",
        "\n",
        "        mean_embeddings.append(mean_embedding.cpu())\n",
        "\n",
        "# numpy に変換してコサイン類似度を計算\n",
        "mean_embeddings = torch.stack(mean_embeddings).numpy()\n",
        "similarities = cosine_similarity(mean_embeddings)\n",
        "\n",
        "# 出力\n",
        "pairs = list(itertools.combinations(range(len(sentences)), 2))\n",
        "for i, j in pairs:\n",
        "    print(f\"Similarity between:\\n  \\\"{sentences[i]}\\\"\\n  \\\"{sentences[j]}\\\"\\n  => {similarities[i][j]:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJqVi0SZx4QO",
        "outputId": "f6c5ac1d-9e24-45be-edef-fff2e8f8de8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of excitement.\"\n",
            "  => 0.9568\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.8490\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of fun.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.8169\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of crap.\"\n",
            "  => 0.8352\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of excitement.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.7938\n",
            "\n",
            "Similarity between:\n",
            "  \"The movie was full of crap.\"\n",
            "  \"The movie was full of rubbish.\"\n",
            "  => 0.9226\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s934DWT_1kFm"
      },
      "source": [
        "## 85. データセットの準備\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) から訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、さらに全てのテキストはトークン列に変換せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kSd-ijqyCrV",
        "outputId": "a011a09b-2193-4c66-c79c-29f3e99ad2db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-21 12:40:21--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.238.176.115, 18.238.176.44, 18.238.176.19, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.238.176.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7439277 (7.1M) [application/zip]\n",
            "Saving to: ‘SST-2.zip’\n",
            "\n",
            "SST-2.zip           100%[===================>]   7.09M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-21 12:40:22 (54.0 MB/s) - ‘SST-2.zip’ saved [7439277/7439277]\n",
            "\n",
            "Archive:  SST-2.zip\n",
            "   creating: SST-2/\n",
            "  inflating: SST-2/dev.tsv           \n",
            "   creating: SST-2/original/\n",
            "  inflating: SST-2/original/README.txt  \n",
            "  inflating: SST-2/original/SOStr.txt  \n",
            "  inflating: SST-2/original/STree.txt  \n",
            "  inflating: SST-2/original/datasetSentences.txt  \n",
            "  inflating: SST-2/original/datasetSplit.txt  \n",
            "  inflating: SST-2/original/dictionary.txt  \n",
            "  inflating: SST-2/original/original_rt_snippets.txt  \n",
            "  inflating: SST-2/original/sentiment_labels.txt  \n",
            "  inflating: SST-2/test.tsv          \n",
            "  inflating: SST-2/train.tsv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('SST-2/train.tsv', sep='\\t')\n",
        "dev_data = pd.read_csv('SST-2/dev.tsv', sep='\\t')\n",
        "\n",
        "print(train_data.head())\n",
        "\n",
        "train_data1 = []\n",
        "for _,j in train_data.iterrows():\n",
        "  tokens = tokenizer.tokenize(j[\"sentence\"])\n",
        "  data = {\"sentence\":tokens,\"label\":j[\"label\"]}\n",
        "  train_data1.append(data)\n",
        "\n",
        "dev_data1 = []\n",
        "for _,j in dev_data.iterrows():\n",
        "  tokens = tokenizer.tokenize(j[\"sentence\"])\n",
        "  data = {\"sentence\":tokens,\"label\":j[\"label\"]}\n",
        "  dev_data1.append(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WAWJHpOyIzB",
        "outputId": "70ec0303-d868-4931-b007-d57c00b22c5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sentence  label\n",
            "0       hide new secretions from the parental units       0\n",
            "1               contains no wit , only labored gags       0\n",
            "2  that loves its characters and communicates som...      1\n",
            "3  remains utterly satisfied to remain the same t...      0\n",
            "4  on the worst revenge-of-the-nerds clichés the ...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiXmLuWpD3_Q"
      },
      "source": [
        "## 86. ミニバッチの作成\n",
        "\n",
        "85で読み込んだ訓練データの一部（例えば冒頭の4事例）に対して、パディングなどの処理を行い、トークン列の長さを揃えてミニバッチを構成せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 86の解答：冒頭4事例に対してパディングを行い、ミニバッチを構成する\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# トークナイザーの準備（BERTを例とする）\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 冒頭の4事例だけを対象にする\n",
        "train_data_subset = train_data1[:4]\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "\n",
        "for i in train_data_subset:\n",
        "    # トークナイズしてID化、特殊トークン追加、attention mask生成\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        \" \".join(i['sentence']),\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids.append(encoded['input_ids'].squeeze(0))\n",
        "    attention_masks.append(encoded['attention_mask'].squeeze(0))\n",
        "    labels.append(int(i['label']))\n",
        "\n",
        "# パディングしてバッチ化（最大長に揃える）\n",
        "input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Input IDs (padded):\")\n",
        "print(input_ids_padded)\n",
        "print(\"\\nAttention Masks:\")\n",
        "print(attention_masks_padded)\n",
        "print(\"\\nLabels:\")\n",
        "print(labels_tensor)\n"
      ],
      "metadata": {
        "id": "OUdgXTjy0er3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48b12f7-9ba4-4c7b-a8cc-47479c1b6243"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs (padded):\n",
            "tensor([[  101,  5342,  2047,  3595,  1001,  1001, 15956,  2013,  1996, 18643,\n",
            "          3197,   102,     0,     0,     0,     0,     0],\n",
            "        [  101,  3397,  2053, 15966,  1010,  2069,  4450,  1001,  1001,  3968,\n",
            "         18201,  1001,  1001,  1055,   102,     0,     0],\n",
            "        [  101,  2008,  7459,  2049,  3494,  1998, 10639,  1001,  1001,  1055,\n",
            "          2242,  2738,  3376,  2055,  2529,  3267,   102],\n",
            "        [  101,  3464, 12580,  8510,  2000,  3961,  1996,  2168,  2802,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "Attention Masks:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
            "\n",
            "Labels:\n",
            "tensor([0, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4nrV6bqD-m9"
      },
      "source": [
        "## 87. ファインチューニング\n",
        "\n",
        "訓練セットを用い、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ml9z8G5r5TTo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OqDN0G2UqFJ"
      },
      "source": [
        "## 88. 極性分析\n",
        "\n",
        "問題87でファインチューニングされたモデルを用いて、以下の文の極性を予測せよ。\n",
        "\n",
        "- \"The movie was full of incomprehensibilities.\"\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EruVW7JaapIv"
      },
      "source": [
        "## 89. アーキテクチャの変更\n",
        "\n",
        "問題87とは異なるアーキテクチャ（例えば[CLS]トークンを用いるか、各トークンの最大値プーリングを用いるなど）の分類モデルを設計し、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}