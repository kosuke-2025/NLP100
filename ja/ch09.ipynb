{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "1HhY1_KNjX_Y",
        "tags": []
      },
      "source": [
        "# 第9章: 事前学習済み言語モデル（BERT型）\n",
        "\n",
        "本章では、BERT型の事前学習済みモデルを利用して、マスク単語の予測や文ベクトルの計算、評判分析器（ポジネガ分類器）の構築に取り組む。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyLKl_eo_80z"
      },
      "source": [
        "## 80. トークン化\n",
        "\n",
        "\"The movie was full of incomprehensibilities.\"という文をトークンに分解し、トークン列を表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = \"The movie was full of incomprehensibilities.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "MUtCt8K5F9sW",
        "outputId": "9f01eeda-7861-4b5a-eb3b-2c74609c67ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'movie', 'was', 'full', 'of', 'inc', '##omp', '##re', '##hen', '##si', '##bilities', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lbx12UDACt6"
      },
      "source": [
        "## 81. マスクの予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"を埋めるのに最も適切なトークンを求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# fill-mask パイプラインの準備（BERT使用）\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# 文に1つだけ [MASK] を含める\n",
        "text = \"The movie was full of [MASK].\"\n",
        "\n",
        "# top_k=1 で最も適切な1語を取得\n",
        "result = unmasker(text, top_k=1)[0]\n",
        "\n",
        "# 結果の表示\n",
        "token = result[\"token_str\"]\n",
        "score = result[\"score\"]\n",
        "\n",
        "print(f\"最も適切な語: {token}\")\n",
        "print(f\"確率: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "S19co1BfGpzW",
        "outputId": "8c4579e6-0ab9-43fe-9312-6669ec875f98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最も適切な語: fun\n",
            "確率: 0.1071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F39DStXDk3OG"
      },
      "source": [
        "## 82. マスクのtop-k予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"に埋めるのに適切なトークン上位10個と、その確率（尤度）を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipelineを作成（マスク補完用）\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# 入力文（[MASK]は必ず大文字で）\n",
        "text = \"The movie was full of [MASK].\"\n",
        "\n",
        "# top_k=10で上位10個の予測を取得\n",
        "results = unmasker(text, top_k=10)\n",
        "\n",
        "# 結果の表示\n",
        "for i, result in enumerate(results, 1):\n",
        "    token = result[\"token_str\"]\n",
        "    score = result[\"score\"]\n",
        "    print(f\"{i}. {token:<15} (probability: {score:.4f})\")\n"
      ],
      "metadata": {
        "id": "yqVDW-wwIvwX",
        "outputId": "e4b60acf-39be-438f-b627-dacf6064ddf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. fun             (probability: 0.1071)\n",
            "2. surprises       (probability: 0.0663)\n",
            "3. drama           (probability: 0.0447)\n",
            "4. stars           (probability: 0.0272)\n",
            "5. laughs          (probability: 0.0254)\n",
            "6. action          (probability: 0.0195)\n",
            "7. excitement      (probability: 0.0190)\n",
            "8. people          (probability: 0.0183)\n",
            "9. tension         (probability: 0.0150)\n",
            "10. music           (probability: 0.0146)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zr6VVYiRPzk"
      },
      "source": [
        "## 83. CLSトークンによる文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqo5Ufzkyc89"
      },
      "source": [
        "## 84. 平均による文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s934DWT_1kFm"
      },
      "source": [
        "## 85. データセットの準備\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) から訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、さらに全てのテキストはトークン列に変換せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiXmLuWpD3_Q"
      },
      "source": [
        "## 86. ミニバッチの作成\n",
        "\n",
        "85で読み込んだ訓練データの一部（例えば冒頭の4事例）に対して、パディングなどの処理を行い、トークン列の長さを揃えてミニバッチを構成せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4nrV6bqD-m9"
      },
      "source": [
        "## 87. ファインチューニング\n",
        "\n",
        "訓練セットを用い、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OqDN0G2UqFJ"
      },
      "source": [
        "## 88. 極性分析\n",
        "\n",
        "問題87でファインチューニングされたモデルを用いて、以下の文の極性を予測せよ。\n",
        "\n",
        "- \"The movie was full of incomprehensibilities.\"\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EruVW7JaapIv"
      },
      "source": [
        "## 89. アーキテクチャの変更\n",
        "\n",
        "問題87とは異なるアーキテクチャ（例えば[CLS]トークンを用いるか、各トークンの最大値プーリングを用いるなど）の分類モデルを設計し、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}